# Per device batch size for training.
per_device_batch_size: 32
# Per device batch size for training.
eval_per_device_batch_size: 32
# Sampling temperature for language model inference.
sampling_temperature: 0.6
# Top k cutoff for logit sampling. If 0 then no top-k cutoff is used.
sampling_top_k: 20
num_train_steps: 500_000
# Number of steps to take during evaluation. Large enough to evaluate all.
# Large enough to evaluate all samples: 306_688 / (32 * 8) = 1198
num_eval_steps: 2_000
# Number of steps to generate predictions.
# -1 will use the whole eval dataset.
num_predict_steps: -1
# Base learning rate.
learning_rate: 0.0016
# Linear learning rate warmup.
warmup_steps: 1000
# Cross entropy loss label smoothing.
label_smoothing: 0.0
# Decay factor for AdamW style weight decay.
weight_decay: 0.1
# Maximum length cutoff for training examples.
max_target_length: 128
# Maximum length cutoff for eval examples.
max_eval_target_length: 512
# Maximum length cutoff for predicted tokens.
max_predict_length: 50
# Final logit transform uses embedding matrix transpose.
logits_via_embedding: False
# Number of transformer layers.
num_layers: 6
# Size of query/key/value for attention.
qkv_dim: 512
# Size of embeddings.
emb_dim: 512
# Size of the MLP.
mlp_dim: 2048
# Number of attention heads.
num_heads: 8
# Dropout rate.
dropout_rate: 0.1
# Attention dropout rate.
attention_dropout_rate: 0.1
# Whether to save model checkpoints.
save_checkpoints: True
# Whether to restore from existing model checkpoints.
restore_checkpoints: True
# Save a checkpoint every these number of steps.
checkpoint_every_steps: 10_000
# Frequency of eval during training, e.g. every 1_000 steps.
eval_every_steps: 1_000
# Use bfloat16 mixed precision training instead of float32.
use_bfloat16: True
# Integer for PRNG random seed.
seed: 0
# Prompt for language model sampling.
prompts: "I love to "

context_size: 20
gamma: 0.9
height: 5
width: 5
